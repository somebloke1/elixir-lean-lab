name: VM Builder Test

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch: # Allow manual triggering

permissions:
  contents: read

jobs:
  test-alpine-builder:
    name: Test Alpine VM Builder
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Elixir
      uses: erlef/setup-beam@v1
      with:
        elixir-version: '1.15.7'
        otp-version: '26.2.1'
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: deps
        key: ${{ runner.os }}-mix-${{ hashFiles('**/mix.lock') }}
        restore-keys: ${{ runner.os }}-mix-
    
    - name: Install dependencies
      run: mix deps.get
    
    - name: Compile project
      run: mix compile
    
    - name: Run basic tests
      run: mix test
    
    - name: Test configuration system
      run: |
        echo "Testing VM configuration..."
        mix run -e "
          config = ElixirLeanLab.configure(type: :alpine, target_size: 25)
          IO.inspect(config, label: \"Config\")
          {:ok, config} = ElixirLeanLab.Config.validate(config)
          IO.puts(\"✅ Configuration validation passed\")
        "
    
    - name: Test OTP stripping configuration
      run: |
        echo "Testing OTP stripper..."
        mix run -e "
          apps = ElixirLeanLab.OTPStripper.applications_to_remove()
          {savings_mb, count} = ElixirLeanLab.OTPStripper.estimate_savings()
          IO.puts(\"Apps to remove: #{count}\")
          IO.puts(\"Estimated savings: #{savings_mb} MB\")
        "
    
    - name: Test kernel configuration
      run: |
        echo "Testing kernel config generation..."
        mix run -e "
          config_script = ElixirLeanLab.KernelConfig.generate_config_script(:qemu_minimal)
          IO.puts(\"✅ Kernel config script generated (#{String.length(config_script)} chars)\")
        "
    
    # Only run Docker tests on push to main (not PRs) to avoid Docker setup complexity
    - name: Test Alpine builder (Docker required)
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      run: |
        echo "Testing Alpine builder with Docker..."
        echo "NOTE: This test requires Docker but may be skipped in CI"
        # Only test configuration, not actual Docker build
        mix run -e "
          try do
            config = ElixirLeanLab.Config.new(type: :alpine, target_size: 25, output: \"./ci-test\")
            {:ok, {builder, config}} = ElixirLeanLab.Builder.new(config)
            IO.puts(\"✅ Alpine builder initialization successful\")
            IO.puts(\"Builder: #{inspect(builder)}\")
          rescue
            e -> IO.puts(\"⚠️  Builder test skipped: #{Exception.message(e)}\")
          end
        "
    
    - name: Test benchmark functionality
      run: |
        echo "Testing benchmark system..."
        # Create a dummy image file for testing
        mkdir -p test-output
        echo "dummy content" > test-output/test-vm.tar
        mix run -e "
          # Test benchmark analysis (should handle missing Docker gracefully)
          result = ElixirLeanLab.VM.analyze(\"test-output/test-vm.tar\")
          IO.inspect(result, label: \"Analysis\")
          IO.puts(\"✅ VM analysis completed\")
        "
    
    - name: Test CLI scripts
      run: |
        echo "Testing CLI scripts..."
        chmod +x scripts/*.sh
        ./scripts/build_vm.sh --help
        ./scripts/benchmark_vm.sh --help
        echo "✅ CLI scripts are executable and show help"
    
    - name: Create and analyze example configuration
      run: |
        echo "Creating VM configuration examples..."
        mix run -e "
          # Test different configurations
          configs = [
            ElixirLeanLab.configure(type: :alpine, target_size: 20),
            ElixirLeanLab.configure(type: :buildroot, target_size: 15),
            ElixirLeanLab.configure(type: :nerves, target_size: 18),
            ElixirLeanLab.configure(type: :custom, target_size: 12)
          ]
          
          Enum.each(configs, fn config ->
            json = ElixirLeanLab.Config.to_json(config)
            parsed = Jason.decode!(json)
            IO.puts(\"#{config.type}: Target #{config.target_size}MB\")
          end)
          
          IO.puts(\"✅ All builder configurations tested\")
        "

  integration-test:
    name: Integration Test with Example App
    runs-on: ubuntu-latest
    needs: test-alpine-builder
    timeout-minutes: 20
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Elixir
      uses: erlef/setup-beam@v1
      with:
        elixir-version: '1.15.7'
        otp-version: '26.2.1'
    
    - name: Install dependencies
      run: mix deps.get && mix compile
    
    - name: Test hello_world example compilation
      working-directory: examples/hello_world
      run: |
        echo "Testing example application..."
        mix deps.get
        mix compile
        echo "✅ Hello world example compiles successfully"
    
    - name: Test full build pipeline (without Docker)
      run: |
        echo "Testing build pipeline..."
        mix run -e "
          # Test the full pipeline without actually building Docker images
          config = ElixirLeanLab.configure(
            type: :alpine,
            target_size: 25,
            app: \"./examples/hello_world\",
            output: \"./ci-build\"
          )
          
          # Test configuration and setup
          {:ok, build_dir} = ElixirLeanLab.Builder.prepare_build_env(config)
          {:ok, app_dir} = ElixirLeanLab.Builder.prepare_app(config.app_path, build_dir)
          
          IO.puts(\"✅ Build environment prepared\")
          IO.puts(\"Build dir: #{build_dir}\")
          IO.puts(\"App copied to: #{app_dir}\")
          
          # Check that files were copied correctly
          if File.exists?(Path.join(app_dir, \"mix.exs\")) do
            IO.puts(\"✅ Application files copied successfully\")
          else
            raise \"Application files not copied correctly\"
          end
        "
    
    - name: Performance and size analysis
      run: |
        echo "Testing performance analysis tools..."
        # Create some dummy VM files for size comparison
        mkdir -p benchmark-test
        
        # Create files of different sizes to simulate different VM types
        dd if=/dev/zero of=benchmark-test/alpine-vm.tar bs=1M count=25 2>/dev/null
        dd if=/dev/zero of=benchmark-test/buildroot-vm.tar bs=1M count=18 2>/dev/null
        dd if=/dev/zero of=benchmark-test/custom-vm.tar bs=1M count=12 2>/dev/null
        
        mix run -e "
          # Test benchmark comparison
          images = [
            \"benchmark-test/alpine-vm.tar\",
            \"benchmark-test/buildroot-vm.tar\", 
            \"benchmark-test/custom-vm.tar\"
          ]
          
          results = ElixirLeanLab.Benchmark.compare(images)
          ElixirLeanLab.Benchmark.generate_report(results, \"ci-benchmark-report.md\")
          
          IO.puts(\"✅ Benchmark report generated\")
        "
    
    - name: Upload benchmark report
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-report
        path: ci-benchmark-report.md
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: build-artifacts
        path: ci-build/
        if-no-files-found: ignore

  check-documentation:
    name: Documentation Check
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Check documentation completeness
      run: |
        echo "Checking documentation..."
        
        # Check that key files exist
        files_to_check=(
          "README.md"
          "docs/ARCHITECTURE.md"
          "docs/PROJECT_STATUS.md"
          "docs/HANDOFF_NOTES.md"
          "examples/hello_world/mix.exs"
        )
        
        for file in "${files_to_check[@]}"; do
          if [ -f "$file" ]; then
            echo "✅ $file exists"
          else
            echo "❌ $file missing"
            exit 1
          fi
        done
        
        # Check that scripts are executable
        if [ -x "scripts/build_vm.sh" ] && [ -x "scripts/benchmark_vm.sh" ]; then
          echo "✅ Scripts are executable"
        else
          echo "❌ Scripts not executable"
          exit 1
        fi
        
        echo "✅ Documentation check passed"